{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28853361",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size:24px;\"><b>Data Cleaning</b></p>\n",
    "\n",
    "A major part of data science is not building models but preparing data so that those models can work properly. In fact, most of the effort goes into cleaning and organizing raw information before any analysis takes place. Raw data often arrives messy, with inconsistencies and formats that make it hard to use directly. To make it usable, data scientists diagnose how “tidy” it is, reshape rows and columns so they line up with the questions being asked, and sometimes merge multiple sources together. They may need to correct value types, such as converting numbers that were stored as text, or handle gaps by dropping or filling missing entries. Another frequent task is manipulating strings to make the data more consistent and meaningful. All these steps are essential because clean, well-structured data is the foundation for reliable insights. Through practice on untidy datasets, one learns the key techniques that transform raw information into a state ready for exploration and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b416212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e7763",
   "metadata": {},
   "source": [
    "# Diagnose the data\n",
    "Tidy data is the standard we aim for when preparing information for analysis. In tidy form, every column represents one variable and every row represents one observation. This structure makes it easy to filter, aggregate, and visualize results. When data isn’t tidy, it often has values spread across columns or compressed into a single field, which makes analysis clumsy. For instance, a wide table with separate columns for “Checkings” and “Savings” balances is harder to work with than a long-form table where account type is a variable and each balance has its own row. By reshaping into the long form, you make the structure consistent and flexible.\n",
    "Before reshaping or cleaning, the first step is diagnosing the state of your dataset. With pandas, you can quickly get a sense of its shape and issues: `.head()` shows the first rows to check for structure, `.info()` summarizes data types and missing values, `.describe()` provides statistical overviews, `.columns` lists the headers, and `.value_counts()` reveals unique values in a column. These functions help you detect untidy patterns, inconsistencies, or errors so you can plan the cleaning steps needed to transform the data into a tidy, usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf4bb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Grocery Item  Cake Recipe  Pancake Recipe  Cookie Recipe\n",
      "0         Eggs            2               3              1\n",
      "1         Milk            1               2              1\n",
      "2        Flour            2               1              2\n",
      "  Grocery Item          Recipe  Number\n",
      "0         Eggs     Cake Recipe       2\n",
      "1         Milk     Cake Recipe       1\n",
      "2        Flour     Cake Recipe       2\n",
      "3         Eggs  Pancake Recipe       3\n",
      "4         Milk  Pancake Recipe       2\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Grocery Item    3 non-null      object\n",
      " 1   Cake Recipe     3 non-null      int64 \n",
      " 2   Pancake Recipe  3 non-null      int64 \n",
      " 3   Cookie Recipe   3 non-null      int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 224.0+ bytes\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9 entries, 0 to 8\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Grocery Item  9 non-null      object\n",
      " 1   Recipe        9 non-null      object\n",
      " 2   Number        9 non-null      int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 344.0+ bytes\n",
      "None\n",
      "       Cake Recipe  Pancake Recipe  Cookie Recipe\n",
      "count     3.000000             3.0       3.000000\n",
      "mean      1.666667             2.0       1.333333\n",
      "std       0.577350             1.0       0.577350\n",
      "min       1.000000             1.0       1.000000\n",
      "25%       1.500000             1.5       1.000000\n",
      "50%       2.000000             2.0       1.000000\n",
      "75%       2.000000             2.5       1.500000\n",
      "max       2.000000             3.0       2.000000\n",
      "         Number\n",
      "count  9.000000\n",
      "mean   1.666667\n",
      "std    0.707107\n",
      "min    1.000000\n",
      "25%    1.000000\n",
      "50%    2.000000\n",
      "75%    2.000000\n",
      "max    3.000000\n",
      "Grocery Item  Cake Recipe  Pancake Recipe  Cookie Recipe\n",
      "Eggs          2            3               1                1\n",
      "Flour         2            1               2                1\n",
      "Milk          1            2               1                1\n",
      "Name: count, dtype: int64\n",
      "Grocery Item  Recipe          Number\n",
      "Eggs          Cake Recipe     2         1\n",
      "              Cookie Recipe   1         1\n",
      "              Pancake Recipe  3         1\n",
      "Flour         Cake Recipe     2         1\n",
      "              Cookie Recipe   2         1\n",
      "              Pancake Recipe  1         1\n",
      "Milk          Cake Recipe     1         1\n",
      "              Cookie Recipe   1         1\n",
      "              Pancake Recipe  2         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV files from the new path \"datafolder/\"\n",
    "df1 = pd.read_csv(\"data_folder/df1.csv\")   # Load df1.csv from the datafolder directory\n",
    "df2 = pd.read_csv(\"data_folder/df2.csv\")   # Load df2.csv from the datafolder directory\n",
    "\n",
    "# Display the first 5 rows of each DataFrame\n",
    "print(df1.head())  # Quick check of df1 structure\n",
    "print(df2.head())  # Quick check of df2 structure\n",
    "\n",
    "# Display info summary (columns, data types, non-null counts)\n",
    "print(df1.info())  # Diagnose df1\n",
    "print(df2.info())  # Diagnose df2\n",
    "\n",
    "# Display descriptive statistics (mean, std, min, max, etc.)\n",
    "print(df1.describe())  # Summary stats for df1\n",
    "print(df2.describe())  # Summary stats for df2\n",
    "\n",
    "# Display distinct rows and their frequencies\n",
    "print(df1.value_counts())  # Value counts for unique rows in df1\n",
    "print(df2.value_counts())  # Value counts for unique rows in df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e53a1",
   "metadata": {},
   "source": [
    "# Dealing with multiples files\n",
    "When working in real-world projects, data often comes split into many separate files that share the same structure. An efficient way to handle this situation is to combine them into a single dataset so that analysis can be done on the whole rather than on fragments. For example, imagine you have files named `file1.csv`, `file2.csv`, `file3.csv`, and so on. Rather than reading them one by one and joining them manually, you can use the `glob` library to automatically collect all the filenames that follow a certain pattern.\n",
    "By writing `glob.glob(\"file*.csv\")`, Python returns a list of all files in the directory whose names start with “file” and end in `.csv`. You can then loop over this list, use `pandas.read_csv()` to read each file into a DataFrame, and store each DataFrame in a list. Finally, `pd.concat()` merges all those DataFrames into a single table.\n",
    "This approach means you don’t have to hard-code every filename; you just tell Python what the pattern looks like, and it handles the rest. The result is one combined DataFrame ready for cleaning, exploration, and analysis, no matter how many files you start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7152c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id         full_name gender_age fractions probability       grade\n",
      "0    0  Nikolia Rainsdon        F15       66%         74%  11th grade\n",
      "1    1    Libbie MacIver        F15       71%         83%  10th grade\n",
      "2    2    Caesar Arnison        M16       70%         78%  12th grade\n",
      "3    3     Leanora Cowup        F15       72%         74%   9th grade\n",
      "4    4     Leanora Cowup        F15       72%         74%   9th grade\n",
      "..  ..               ...        ...       ...         ...         ...\n",
      "95  95  Brig Meadowcroft        M16       84%         NaN  12th grade\n",
      "96  96  Isidor Abrashkov        M14       83%         78%  11th grade\n",
      "97  97      Siana McKune        F17       72%         76%  12th grade\n",
      "98  98       Tuck Lyford        M17       73%         84%  11th grade\n",
      "99  99       Janie Paris        F17       68%         82%  11th grade\n",
      "\n",
      "[600 rows x 6 columns]\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd      # pandas is used to work with tabular data\n",
    "import glob              # glob is used to find filenames matching a pattern\n",
    "\n",
    "# student_files is a list of all filenames that match \"exams*.csv\"\n",
    "# Example: [\"exams0.csv\", \"exams1.csv\", ..., \"exams9.csv\"]\n",
    "student_files = glob.glob(\"data_folder/exams*.csv\")\n",
    "\n",
    "# df_list is an empty Python list that will store multiple DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Loop through each filename in student_files\n",
    "for filename in student_files:\n",
    "    # df is a pandas DataFrame created by reading the CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "    # Append each DataFrame to df_list\n",
    "    df_list.append(df)\n",
    "\n",
    "# students is a single pandas DataFrame created by concatenating\n",
    "# all the DataFrames stored inside df_list\n",
    "students = pd.concat(df_list)\n",
    "\n",
    "# Print the combined DataFrame\n",
    "print(students)\n",
    "\n",
    "# Print the number of rows (observations) in the combined DataFrame\n",
    "print(len(students))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb902b1",
   "metadata": {},
   "source": [
    "# Reshaping your Data\n",
    "Reshaping data is often necessary to turn a dataset into tidy form, where every variable has its own column and every observation has its own row. A common issue is when different categories of the same variable are stored as separate columns. For instance, you might see account balances spread across “Checking” and “Savings” columns, which makes comparisons and aggregations awkward. The tidy version has a single “Account Type” column that indicates whether a row is a checking or savings balance, and a single “Amount” column that records the balance.\n",
    "The tool for this transformation in pandas is `.melt()`. It “unpivots” wide data into long format. You specify which column(s) should remain fixed (`id_vars`), which columns you want to transform into variable/value pairs (`value_vars`), what to call the new variable column (`var_name`), and what to call the new values column (`value_name`). For example, calling `.melt()` on the account table with “Account” as the ID and “Checking” and “Savings” as the value variables produces a long table where each row is one balance, clearly identified by account number and account type.\n",
    "This tidy structure is far more flexible. It makes it straightforward to group by account type, calculate totals, or merge with other datasets. After melting, it’s also common to rename the columns so they are descriptive, ensuring that the table is self-explanatory to anyone who uses it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ad681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'full_name', 'gender_age', 'fractions', 'probability',\n",
      "       'grade'],\n",
      "      dtype='object')\n",
      "-----------------------------------------------------\n",
      "           full_name gender_age       grade       exam score\n",
      "0     Moses Kirckman        M14  11th grade  fractions   69%\n",
      "1    Timofei Strowan        M18  11th grade  fractions   63%\n",
      "2       Silvain Poll        M18   9th grade  fractions   69%\n",
      "3     Lezley Pinxton        M18  11th grade  fractions   NaN\n",
      "4  Bernadene Saunper        F17  11th grade  fractions   72%\n",
      "-----------------------------------------------------\n",
      "Index(['full_name', 'gender_age', 'grade', 'exam', 'score'], dtype='object')\n",
      "-----------------------------------------------------\n",
      "exam\n",
      "fractions      1000\n",
      "probability    1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "students = pd.read_csv(\"data_folder/students.csv\")\n",
    "\n",
    "# 1. Print the column names of the original students DataFrame\n",
    "print(students.columns)   # This shows what columns exist before reshaping\n",
    "\n",
    "# 2. Reshape the DataFrame with pd.melt()\n",
    "# - Keep 'full_name', 'gender_age', and 'grade' as identifier columns\n",
    "# - Melt the exam score columns: 'fractions' and 'probability'\n",
    "# - Create a new column 'exam' that stores the exam type\n",
    "# - Create a new column 'score' that stores the actual exam score\n",
    "students = pd.melt(\n",
    "    frame=students,\n",
    "    id_vars=['full_name', 'gender_age', 'grade'],\n",
    "    value_vars=['fractions', 'probability'],\n",
    "    value_name='score',\n",
    "    var_name='exam'\n",
    ")\n",
    "print(\"-----------------------------------------------------\")\n",
    "# 3. Print out the first rows to verify reshaping worked\n",
    "print(students.head())\n",
    "print(\"-----------------------------------------------------\")\n",
    "# 4. Print the new column names\n",
    "print(students.columns)\n",
    "print(\"-----------------------------------------------------\")\n",
    "# 5. Print the counts of each exam type\n",
    "print(students['exam'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f7599",
   "metadata": {},
   "source": [
    "# Dealing with Duplicates\n",
    "Duplicates are a common issue in datasets, often caused by mistakes during data entry, collection, or saving. They can lead to misleading analyses if not handled properly. In pandas, you can detect duplicates with the `.duplicated()` method, which returns a series of Boolean values indicating whether each row is a repeat of a previous one. A row is considered a duplicate only if every value in that row matches another row exactly.\n",
    "To clean the dataset, `.drop_duplicates()` removes all repeated rows, keeping only the first occurrence. For example, if two identical “apple” rows exist, one will be dropped while unique rows remain. However, rows that differ in even one column (like two “peach” entries with different prices) are treated as distinct and not removed.\n",
    "You can also target duplicates in specific columns by using the `subset` parameter. Calling `.drop_duplicates(subset=[\"item\"])` ensures only the first row for each item is kept, dropping any additional entries, even if other values differ. This is helpful when you know that duplicates in a certain variable (like “item”) don’t add useful information. On the other hand, you wouldn’t want to remove duplicates based on a column like “price,” because multiple items legitimately share the same price.\n",
    "The key is to carefully decide which columns define uniqueness in your context. Used thoughtfully, `.duplicated()` and `.drop_duplicates()` ensure your data is clean, accurate, and reliable for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597d76be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              full_name gender_age       grade         exam score\n",
      "0        Moses Kirckman        M14  11th grade    fractions   69%\n",
      "1       Timofei Strowan        M18  11th grade    fractions   63%\n",
      "2          Silvain Poll        M18   9th grade    fractions   69%\n",
      "3        Lezley Pinxton        M18  11th grade    fractions   NaN\n",
      "4     Bernadene Saunper        F17  11th grade    fractions   72%\n",
      "...                 ...        ...         ...          ...   ...\n",
      "1995     Wilie Stillert        F14   9th grade  probability   69%\n",
      "1996     Gertie Flicker        F15  11th grade  probability   86%\n",
      "1997       Yettie Labes        F14  12th grade  probability   82%\n",
      "1998     Lock McGuinley        M18  10th grade  probability   84%\n",
      "1999       Bebe Lebbern        F15  12th grade  probability   91%\n",
      "\n",
      "[2000 rows x 5 columns]\n",
      "False    1976\n",
      "True       24\n",
      "Name: count, dtype: int64\n",
      "False    1976\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print the original DataFrame\n",
    "print(students)\n",
    "\n",
    "# 1. Create a Series showing which rows are duplicates\n",
    "duplicates = students.duplicated()\n",
    "\n",
    "# 2. Print counts of True/False → tells us how many rows are duplicates\n",
    "print(duplicates.value_counts())\n",
    "\n",
    "# 3. Drop duplicate rows and update the DataFrame\n",
    "students = students.drop_duplicates()\n",
    "\n",
    "# 4. Check again for duplicates after dropping\n",
    "duplicates = students.duplicated()\n",
    "print(duplicates.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82bde6",
   "metadata": {},
   "source": [
    "# Split Column values\n",
    "Sometimes data columns contain multiple pieces of information bundled together, and we need to separate them into distinct variables to make the dataset tidy. A common case is when dates are stored as strings without delimiters, like “MMDDYYYY.” For example, “12241989” represents December 24, 1989. While this format is compact, it makes analysis harder because month, day, and year are not directly accessible.\n",
    "When we know the exact structure of the string, pandas allows us to split it using string indexing. By slicing substrings with `.str`, we can create new columns. Taking the first two characters gives the month, the next two characters give the day, and the rest of the string provides the year. For instance, the value “10311966” becomes month = “10”, day = “31”, and year = “1966.”\n",
    "This transformation turns one messy column into three clear variables that can be used separately in analysis or visualization. It also helps prepare the data for conversion into numeric or datetime formats later. Splitting by index is thus a quick and reliable technique whenever you have strings with a consistent fixed-length structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71281cda",
   "metadata": {},
   "source": [
    "What .str does\n",
    "- In pandas, a column of text values (strings) is stored as a Series.\n",
    "- The .str accessor allows you to apply vectorized string operations directly to every element in the column.\n",
    "- For example, .str[0] takes the first character of every string, .str[1:] takes the substring starting from the second character to the end.\n",
    "- Without .str, you’d need to loop through each row manually, but .str applies the operation efficiently to the whole column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c6d9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              full_name gender_age       grade         exam score\n",
      "0        Moses Kirckman        M14  11th grade    fractions   69%\n",
      "1       Timofei Strowan        M18  11th grade    fractions   63%\n",
      "2          Silvain Poll        M18   9th grade    fractions   69%\n",
      "3        Lezley Pinxton        M18  11th grade    fractions   NaN\n",
      "4     Bernadene Saunper        F17  11th grade    fractions   72%\n",
      "...                 ...        ...         ...          ...   ...\n",
      "1995     Wilie Stillert        F14   9th grade  probability   69%\n",
      "1996     Gertie Flicker        F15  11th grade  probability   86%\n",
      "1997       Yettie Labes        F14  12th grade  probability   82%\n",
      "1998     Lock McGuinley        M18  10th grade  probability   84%\n",
      "1999       Bebe Lebbern        F15  12th grade  probability   91%\n",
      "\n",
      "[1976 rows x 5 columns]\n",
      "Index(['full_name', 'gender_age', 'grade', 'exam', 'score'], dtype='object')\n",
      "0    M14\n",
      "1    M18\n",
      "2    M18\n",
      "3    M18\n",
      "4    F17\n",
      "Name: gender_age, dtype: object\n",
      "           full_name gender_age       grade       exam score gender age\n",
      "0     Moses Kirckman        M14  11th grade  fractions   69%      M  14\n",
      "1    Timofei Strowan        M18  11th grade  fractions   63%      M  18\n",
      "2       Silvain Poll        M18   9th grade  fractions   69%      M  18\n",
      "3     Lezley Pinxton        M18  11th grade  fractions   NaN      M  18\n",
      "4  Bernadene Saunper        F17  11th grade  fractions   72%      F  17\n",
      "Index(['full_name', 'gender_age', 'grade', 'exam', 'score', 'gender', 'age'], dtype='object')\n",
      "           full_name       grade       exam score gender age\n",
      "0     Moses Kirckman  11th grade  fractions   69%      M  14\n",
      "1    Timofei Strowan  11th grade  fractions   63%      M  18\n",
      "2       Silvain Poll   9th grade  fractions   69%      M  18\n",
      "3     Lezley Pinxton  11th grade  fractions   NaN      M  18\n",
      "4  Bernadene Saunper  11th grade  fractions   72%      F  17\n"
     ]
    }
   ],
   "source": [
    "# 1. Print the original DataFrame\n",
    "print(students)\n",
    "\n",
    "# Print column names to inspect structure\n",
    "print(students.columns)\n",
    "\n",
    "# 2. Inspect the first rows of the gender_age column\n",
    "print(students[\"gender_age\"].head())\n",
    "\n",
    "# 3. Extract the first character as gender\n",
    "students[\"gender\"] = students.gender_age.str[0]\n",
    "\n",
    "# 4. Extract the rest of the string as age\n",
    "students[\"age\"] = students.gender_age.str[1:]\n",
    "\n",
    "# 5. Inspect the DataFrame after adding new columns\n",
    "print(students.head())\n",
    "print(students.columns)\n",
    "\n",
    "# 6. Drop the old gender_age column by selecting only the needed columns\n",
    "students = students[['full_name', 'grade', 'exam', 'score', 'gender', 'age']]\n",
    "print(students.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec06c7ba",
   "metadata": {},
   "source": [
    "# Splitting Strings 2\n",
    "Sometimes a single column contains more than one piece of information, and splitting by fixed positions won’t work if the parts have different lengths. In those cases, we can split by a specific character that separates the values. For example, if a column named type has entries like user_Kenya or admin_US, the underscore _ is the natural divider between user type and country.\n",
    "With pandas, .str.split('_') separates each string into a list: everything before the underscore goes into the first position, and everything after goes into the second. By saving this intermediate result, we can then select the pieces we want. Using .str.get(0) picks the first part (e.g., user, admin), and .str.get(1) picks the second part (e.g., Kenya, US).\n",
    "This process reshapes the dataset into a tidier form with two new columns, usertype and country, which makes it much easier to analyze patterns such as how many users come from each country or how behavior differs across roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6867e29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           full_name       grade       exam score gender age first_name  \\\n",
      "0     Moses Kirckman  11th grade  fractions   69%      M  14      Moses   \n",
      "1    Timofei Strowan  11th grade  fractions   63%      M  18    Timofei   \n",
      "2       Silvain Poll   9th grade  fractions   69%      M  18    Silvain   \n",
      "3     Lezley Pinxton  11th grade  fractions   NaN      M  18     Lezley   \n",
      "4  Bernadene Saunper  11th grade  fractions   72%      F  17  Bernadene   \n",
      "\n",
      "  last_name  \n",
      "0  Kirckman  \n",
      "1   Strowan  \n",
      "2      Poll  \n",
      "3   Pinxton  \n",
      "4   Saunper  \n"
     ]
    }
   ],
   "source": [
    "# 1. Split the 'full_name' column into two parts wherever there is a space \" \"\n",
    "name_split = students[\"full_name\"].str.split(\" \")\n",
    "\n",
    "# 2. Take the first part (index 0) as the first name\n",
    "students[\"first_name\"] = name_split.str.get(0)\n",
    "\n",
    "# 3. Take the second part (index 1) as the last name\n",
    "students[\"last_name\"] = name_split.str.get(1)\n",
    "\n",
    "# 4. Print the first rows to check the new structure\n",
    "print(students.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3f25f",
   "metadata": {},
   "source": [
    "# Looking at Types\n",
    "Every column in a pandas DataFrame has a data type, known as a **dtype**, and the dtype determines what kinds of operations are possible. Columns can be numeric (like `int` or `float`), boolean, dates or times (`datetime`, `timedelta`), categories, or generic objects (often strings). When data comes in from a CSV or other raw source, pandas may store some values as objects even if they look numeric, which can make analysis harder.\n",
    "\n",
    "For example, if a column of prices is stored as strings like `\"$3\"`, the dtype will be `object`. That means you can’t directly calculate averages or plot trends because pandas treats the column as text rather than numbers. By contrast, a column of calorie counts stored as `int64` can be summed, averaged, or plotted without issue.\n",
    "\n",
    "To check the types of all columns in your DataFrame, you use `.dtypes`. This returns a Series object listing the dtype of each column. For instance, in a table with fruit names, prices stored as strings, and calorie counts as integers, `.dtypes` would show `object` for the text columns and `int64` for the calorie column.\n",
    "\n",
    "Knowing your dtypes is the first step in cleaning because it tells you which columns need conversion before analysis. The next steps often involve transforming object columns into numeric or datetime types so you can apply mathematical or time-based operations to them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d374d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_name     object\n",
      "grade         object\n",
      "exam          object\n",
      "score         object\n",
      "gender        object\n",
      "age           object\n",
      "first_name    object\n",
      "last_name     object\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(students\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 2. Try to calculate the mean of the 'score' column\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This will cause an error if 'score' is stored as strings instead of numbers\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstudents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/core/series.py:6560\u001b[0m, in \u001b[0;36mSeries.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6552\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   6553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m   6554\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6558\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   6559\u001b[0m ):\n\u001b[0;32m-> 6560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/core/generic.py:12439\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  12433\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12434\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12437\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12438\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 12439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12440\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  12441\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/core/generic.py:12396\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12392\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[1;32m  12394\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 12396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[1;32m  12398\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/core/series.py:6468\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   6463\u001b[0m     \u001b[38;5;66;03m# GH#47500 - change to TypeError to match other methods\u001b[39;00m\n\u001b[1;32m   6464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   6465\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6466\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6467\u001b[0m     )\n\u001b[0;32m-> 6468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    145\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/core/nanops.py:719\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    716\u001b[0m     dtype_count \u001b[38;5;241m=\u001b[39m dtype\n\u001b[1;32m    718\u001b[0m count \u001b[38;5;241m=\u001b[39m _get_counts(values\u001b[38;5;241m.\u001b[39mshape, mask, axis, dtype\u001b[38;5;241m=\u001b[39mdtype_count)\n\u001b[0;32m--> 719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(the_sum)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/numpy/_core/_methods.py:52\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "# 1. Inspect the data types of each column\n",
    "print(students.dtypes)\n",
    "\n",
    "# 2. Try to calculate the mean of the 'score' column\n",
    "# This will cause an error if 'score' is stored as strings instead of numbers\n",
    "print(students[\"score\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401f20d",
   "metadata": {},
   "source": [
    "# String Parsing\n",
    "String parsing is an important step in cleaning data, especially when numbers are stored as text with extra characters that make them unusable for calculations. A common case is prices stored with dollar signs. While `\"$3\"` looks like a number, pandas reads it as a string because of the symbol, meaning you can’t average or compare values directly.\n",
    "To fix this, you can use regular expressions with `.replace()` to strip out unwanted characters. For example, applying `.replace('[\\$,]', '', regex=True)` to the price column removes dollar signs and commas, leaving only numeric characters in the strings. Once the column contains clean strings, you then use `pd.to_numeric()` to convert it into a numeric dtype, such as float.\n",
    "After these steps, the column can be used for proper analysis: you can compute the mean price, sum totals, or make price comparisons between fruits. This transformation turns the dataset from something visually understandable but computationally awkward into a format that is both tidy and analytically useful.\n",
    "By parsing strings like this, you turn inconsistent, text-based data into reliable numerical variables, a crucial step for accurate analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a21d2fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_name     object\n",
      "grade         object\n",
      "exam          object\n",
      "score         object\n",
      "gender        object\n",
      "age           object\n",
      "first_name    object\n",
      "last_name     object\n",
      "dtype: object\n",
      "full_name      object\n",
      "grade          object\n",
      "exam           object\n",
      "score         float64\n",
      "gender         object\n",
      "age            object\n",
      "first_name     object\n",
      "last_name      object\n",
      "dtype: object\n",
      "           full_name       grade       exam  score gender age first_name  \\\n",
      "0     Moses Kirckman  11th grade  fractions   69.0      M  14      Moses   \n",
      "1    Timofei Strowan  11th grade  fractions   63.0      M  18    Timofei   \n",
      "2       Silvain Poll   9th grade  fractions   69.0      M  18    Silvain   \n",
      "3     Lezley Pinxton  11th grade  fractions    NaN      M  18     Lezley   \n",
      "4  Bernadene Saunper  11th grade  fractions   72.0      F  17  Bernadene   \n",
      "\n",
      "  last_name  \n",
      "0  Kirckman  \n",
      "1   Strowan  \n",
      "2      Poll  \n",
      "3   Pinxton  \n",
      "4   Saunper  \n"
     ]
    }
   ],
   "source": [
    "# From object the score becomes float\n",
    "print(students.dtypes) \n",
    "\n",
    "# 1. Remove the '%' signs from the score column using regex\n",
    "students[\"score\"] = students[\"score\"].replace('[\\%,]', '', regex=True)\n",
    "\n",
    "# 2. Convert the cleaned score column from string (object) to numeric (int/float)\n",
    "students[\"score\"] = pd.to_numeric(students[\"score\"])\n",
    "\n",
    "# Check the result\n",
    "print(students.dtypes)   # confirm 'score' is now numeric\n",
    "print(students.head())   # preview the cleaned DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992e21db",
   "metadata": {},
   "source": [
    "# More String Parsing\n",
    "Sometimes the numbers we need for analysis are hidden inside longer text strings. For example, workout logs might store both the type of exercise and the number of repetitions in the same field: `\"lunges - 30 reps\"`. While this is easy for humans to read, it’s not structured enough for analysis. To make the data usable, we want to separate the exercise type from the number of reps.\n",
    "With pandas, we can do this using `.str.split()` together with a regular expression. Splitting on the pattern `(\\d+)` tells pandas to break the string wherever it finds digits, and because the digits are in parentheses, they are preserved as part of the result. Using `expand=True` creates a temporary DataFrame where the exercise description, number of reps, and leftover text like `\"reps\"` are split into separate columns.\n",
    "Once we have this split DataFrame, we can assign the parts back to the original data. The middle column (the digits) becomes the `reps` column, and we convert it to numeric with `pd.to_numeric()` so we can calculate totals or averages. The first column (the exercise name) can be cleaned with `.replace('[\\- ]', '', regex=True)` to remove stray hyphens or spaces, leaving just the exercise name.\n",
    "The result is a tidy dataset where each row has separate, meaningful variables: `date`, `exercise`, and `reps`. Now we can easily track how many squats or lunges were done over time, compare exercises, and perform all kinds of analysis that would not be possible if everything remained trapped in a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd7f50cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    11th grade\n",
      "1    11th grade\n",
      "2     9th grade\n",
      "3    11th grade\n",
      "4    11th grade\n",
      "Name: grade, dtype: object\n",
      "full_name      object\n",
      "grade          object\n",
      "exam           object\n",
      "score         float64\n",
      "gender         object\n",
      "age            object\n",
      "first_name     object\n",
      "last_name      object\n",
      "dtype: object\n",
      "10.620445344129555\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Prime 5 righe della colonna grade\n",
    "print(students['grade'].head())\n",
    "\n",
    "# 2) Estrai il numero con regex (più diretto di split)\n",
    "students['grade'] = students['grade'].str.extract(r'(\\d+)')\n",
    "\n",
    "# 3) Tipi\n",
    "print(students.dtypes)\n",
    "\n",
    "# 4) Converti a numerico\n",
    "students['grade'] = pd.to_numeric(students['grade'], errors='coerce')\n",
    "\n",
    "# 5) Media\n",
    "avg_grade = students['grade'].mean()\n",
    "print(avg_grade)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e4cec",
   "metadata": {},
   "source": [
    "# Missing Values\n",
    "Missing values are a routine problem in datasets and can occur for many reasons: incomplete data collection, errors in storage, or simple human mistakes. In pandas, these show up as `NaN` (Not a Number), which represents the absence of a value. While some calculations can ignore `NaN`s, others will fail or give misleading results unless the gaps are handled properly.\n",
    "There are two main strategies to deal with missing values:\n",
    "\n",
    "**1. Dropping rows with missing data.**\n",
    "If you don’t want incomplete rows to affect your analysis, you can remove them using `.dropna()`. By default, this deletes any row containing a `NaN`. For more targeted cleaning, you can specify a `subset`, which only removes rows where certain columns are missing. For instance, dropping rows with `NaN` in `num_guests` keeps the rest of the table intact.\n",
    "\n",
    "**2. Filling in missing data.**\n",
    "Sometimes it’s more useful to replace missing values rather than delete them, especially if losing rows would throw away important information. The `.fillna()` method allows you to substitute `NaN`s with chosen values. A common practice is to fill with the mean of the column or another statistic, so the filled values are consistent with the data’s distribution. For example, filling a missing `bill` with the average bill and missing `num_guests` with the average number of guests preserves the dataset size while making it usable for analysis.\n",
    "\n",
    "Choosing between dropping and filling depends on the context: dropping is safer when you want only complete records, while filling is helpful when you need to retain all rows but ensure the data is valid for calculations. Both are essential techniques for preparing clean, reliable datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d297de91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.69657422512235\n",
      "72.30971659919028\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the mean of the score column before filling NaNs\n",
    "score_mean = students[\"score\"].mean()\n",
    "print(score_mean)\n",
    "\n",
    "# 2. Replace all NaN values in the score column with 0\n",
    "students[\"score\"] = students[\"score\"].fillna(0)\n",
    "\n",
    "# 3. Get the mean of the score column again after filling\n",
    "score_mean_2 = students[\"score\"].mean()\n",
    "print(score_mean_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75485d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
